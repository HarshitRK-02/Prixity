import os
import sys
import uvicorn
from dotenv import load_dotenv
from fastapi import FastAPI, WebSocket
from loguru import logger

# --- Robust Imports for Pipecat 0.0.97+ ---
try:
    from pipecat.pipeline.pipeline import Pipeline
    from pipecat.pipeline.runner import PipelineRunner
    from pipecat.pipeline.task import PipelineTask
    from pipecat.processors.aggregators.llm_response import LLMUserResponseAggregator
    from pipecat.services.deepgram.stt import DeepgramSTTService
    from pipecat.services.deepgram.tts import DeepgramTTSService
    from pipecat.services.openai.llm import OpenAILLMService
    from pipecat.transports.websocket.fastapi import FastAPIWebsocketTransport, FastAPIWebsocketParams
    
    # Import Context handling
    # Depending on version, this might be in different spots. We try standard first.
    from pipecat.processors.aggregators.llm_response import LLMContext
except ImportError as e:
    print(f"\nCRITICAL IMPORT ERROR: {e}")
    print("Please run: pip install -U pipecat-ai[deepgram,openai]\n")
    sys.exit(1)

# Import VAD (Voice Activity Detection)
try:
    from pipecat.audio.vad.silence import SilenceVAD
except ImportError:
    try:
        from pipecat.vad.silence import SilenceVAD
    except:
        SilenceVAD = None
        print("Warning: SilenceVAD not found. VAD disabled.")

load_dotenv()

# Enable Debug Logs - This will show you "STT: [Your Words]" in the terminal
logger.remove()
logger.add(sys.stderr, level="DEBUG")

app = FastAPI()

@app.websocket("/ws")
async def websocket_endpoint(websocket: WebSocket):
    await websocket.accept()
    
    # 1. VAD Configuration (Helps detect when you stop speaking)
    vad_args = {"vad_enabled": False}
    if SilenceVAD:
        vad_args = {
            "vad_enabled": True,
            "vad_analyzer": SilenceVAD(
                start_silence_time=0.5, # Wait 0.5s speech to start
                stop_silence_time=0.6   # Wait 0.6s silence to consider "done"
            ), 
            "vad_audio_passthrough": True
        }

    # 2. Transport
    transport = FastAPIWebsocketTransport(
        websocket=websocket,
        params=FastAPIWebsocketParams(
            audio_out_enabled=True,
            add_wav_header=True,
            **vad_args
        )
    )

    # 3. Services
    stt = DeepgramSTTService(api_key=os.getenv("DEEPGRAM_API_KEY"))
    
    llm = OpenAILLMService(
        api_key=os.getenv("GROQ_API_KEY"),
        base_url="https://api.groq.com/openai/v1",
        model="llama3-8b-8192" 
    )

    tts = DeepgramTTSService(
        api_key=os.getenv("DEEPGRAM_API_KEY"),
        voice="aura-asteria-en"
    )

    # 4. Context & Aggregator (THE MISSING LINK)
    # This keeps track of the conversation history
    messages = [
        {
            "role": "system",
            "content": "You are a helpful voice assistant from Pixrity. Keep your answers very short, concise, and conversational."
        }
    ]
    context = LLMContext(messages=messages)
    context_aggregator = LLMUserResponseAggregator(context)

    # 5. Pipeline
    pipeline = Pipeline(
        [
            transport.input(),      # 1. Get Audio
            stt,                    # 2. Audio -> Text
            context_aggregator,     # 3. Text -> LLM Context (Fixes the silence issue)
            llm,                    # 4. Context -> Text Response
            tts,                    # 5. Text -> Audio Response
            transport.output()      # 6. Play Audio
        ]
    )

    task = PipelineTask(
        pipeline,
        params=PipelineTask.PipelineParams(
            allow_interruptions=True
        )
    )

    runner = PipelineRunner()

    print("\n--- Voice Agent Running ---")
    print("Speak into the mic. Check this terminal for STT logs.\n")
    
    await runner.run(task)

if __name__ == "__main__":
    uvicorn.run(app, host="0.0.0.0", port=8765)
